{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar learning in children\n",
    "\n",
    "This notebook simulates data for a study on learning artificial grammar in children.\n",
    "\n",
    "Study design: Daniela Sch√∂nberger\n",
    "Modelling: Clara Kuper\n",
    "\n",
    "*Children are presented with a sequence of elements (colours, animals, fruits) that follow a defined gramatical structure. The grammar is defined in Schiff and Katan (2014), Fig 1C; and in Reber (1967). After learning by passivly watching grammatical sequences participants make a forced choice between two sequences, which of the presented sequences is grammatically valid. If the responses are repeatedly correct, the sequence is elongated resulting in a harder discrimination. If responses are repeatedly incorrect, the sequence is shortened.*\n",
    " \n",
    "###########\n",
    "\n",
    "*Difficulty levels* \n",
    "4 levels: \n",
    "1 = sequences with 3-4 items \n",
    "2 = sequences with 4-5 items \n",
    "3 = sequences with 5-6 items \n",
    "4 = sequences with 6-7 items\n",
    "\n",
    "*Adaptation rule*\n",
    "0-1 trial correct: easier\n",
    "2-3 trials correct: same\n",
    "4-5 trials correct: harder\n",
    "\n",
    "###########\n",
    "\n",
    "#### Open Question:\n",
    "\n",
    "How can we develop a score for the responses that captures the differences in difficulty over different levels?\n",
    "(3 responses correct in level 2 are 'better' than 3 responses correct in level 1)\n",
    "\n",
    "Problems: levels overlap (length 4 appears in level 1 an level 2)\n",
    "\n",
    "*Adaptive scores (ideas)*\n",
    "\n",
    "*Idea 1*\n",
    "score = level - wrong answers/trials\n",
    "summed up / averaged for one session\n",
    "\n",
    "*Idea 2*\n",
    "score = sum over length of \"correctly answered\" trains\n",
    "\n",
    "*Idea 3*\n",
    "comupte difficulty of grammar with n elements\n",
    "\n",
    "score = sum over difficulty of \"correctly answered\" trains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define levels\n",
    "levels = [[3,4],[4,5],[5,6],[6,7]]\n",
    "\n",
    "# Define experiement blocks and trials\n",
    "nblock = 8\n",
    "ntrial = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decimal range function\n",
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a logistic response function from 1 to 0\n",
    "\n",
    "def resp_psychometric(x,k,x0): \n",
    "    'This function returns the probability for a correct response as a logistic response function. \\\n",
    "    It depends on the parameters: \\nx = input value \\nk = slope \\nx0 = midpoint'\n",
    "    fx = 1-(1/(1+(m.e**(-k*(x-x0)))))\n",
    "    fx = 0.5 + 0.5*fx\n",
    "    return fx\n",
    "\n",
    "# a list to nicely plot the function with continuous input values\n",
    "\n",
    "def make_list(start,stop,slope,reflect):\n",
    "    psych_list = []\n",
    "\n",
    "    for v in frange(start,stop,0.1):\n",
    "        psych = resp_psychometric(v,slope,reflect)\n",
    "        psych_list = psych_list + [psych]\n",
    "    \n",
    "    return psych_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that decides if the level remains or not\n",
    "def define_level(data,level):\n",
    "    'depends on the cut-off-values that are being set here'\n",
    "    if data >= 4:\n",
    "        if level < 3:\n",
    "            return level+1\n",
    "        else: \n",
    "            return level\n",
    "    if data <= 1:\n",
    "        if level > 0:\n",
    "            return level-1\n",
    "        else:\n",
    "            return level\n",
    "    else:\n",
    "        return level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class kid\n",
    "class Kid():\n",
    "    # define attributes of the kid\n",
    "    'a class that holds the properties name, age and response parameters \\\n",
    "    psych_curve and psych_reflect. \\\n",
    "    Learning parameters set the logistic response function and should be modulated with learning \\\n",
    "    Suggested range for psych_curve: > 0 to 10  \\\n",
    "    Suggested range for psych_reflection: >= shortest element, <= than longest element'\n",
    "   \n",
    "    def __init__(self, name, age, psych_curve, psych_reflect):\n",
    "        self.name          = name\n",
    "        self.age           = age\n",
    "        self.psych_curve   = psych_curve\n",
    "        self.psych_reflect = psych_reflect\n",
    "\n",
    "    # define testing\n",
    "    def test_block(self, level_test, n_trial):\n",
    "        \n",
    "        # reset the number of correct responses in block\n",
    "        change_level = 0\n",
    "        \n",
    "        # empty data frame to be filled\n",
    "        data = pd.DataFrame('NA', index=range(ntrial), columns=['trial','response', 'length', 'level', 'block'])\n",
    "        \n",
    "        # loop through trials\n",
    "        for trial in range(n_trial):\n",
    "            \n",
    "            # how long is the sequence\n",
    "            length = rd.sample(level_test,1)[0]\n",
    "            \n",
    "            # how difficult is the response based on the length\n",
    "            prob_correct  = resp_psychometric(length,self.psych_curve,self.psych_reflect)\n",
    "            \n",
    "            # get correct (1)/incorrect (0) answer\n",
    "            answer = rd.choices(population=[1,0],weights=[prob_correct,1-prob_correct],k=1)\n",
    "            \n",
    "            # sum over block\n",
    "            change_level = change_level+answer[0]\n",
    "            \n",
    "            # save to output\n",
    "            data.iloc[trial]['trial']      = trial\n",
    "            data.iloc[trial]['response']   = answer[0]\n",
    "            data.iloc[trial]['length']     = length \n",
    "        \n",
    "        return change_level, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiement\n",
    "def run_experiment(nblock, ntrial, kid):\n",
    "\n",
    "    # Start at level one\n",
    "    level = 0\n",
    "    \n",
    "    # Empty data frame\n",
    "    raw_data = pd.DataFrame('NA', index=[], columns=['trial', 'response', 'length', 'level', 'block'])\n",
    "    \n",
    "    # Loop through blocks\n",
    "    for block in range(nblock):\n",
    "        \n",
    "        # each block returns response data and a new level\n",
    "        change_level, new_data = kid.test_block(levels[level],ntrial)\n",
    "        \n",
    "        # save number of the block to output\n",
    "        new_data['block'] = block\n",
    "        new_data['level'] = level\n",
    "        raw_data   = pd.concat([raw_data, new_data])\n",
    "        \n",
    "        # new level for next block\n",
    "        level = define_level(change_level,level)\n",
    "    \n",
    "    # return output         \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "# Define the parameters for the participant:\n",
    "\n",
    "slope_of_function   = 20   # \"0\" flat line, 0 - 1 approx. linear, \n",
    "                          # higher values reflect steeper switch between 0 and 1\n",
    "                          # negative values reverse response\n",
    "point_of_reflection = 4   # 75% correct responses at this length\n",
    "min_length          = 3   # the shortest sequence is that long\n",
    "max_length          = 7   # the longest sequence is that long\n",
    "\n",
    "# show the responses for better overview\n",
    "simulated_list = make_list(min_length,max_length,slope_of_function,point_of_reflection)\n",
    "plt.plot(simulated_list);\n",
    "plt.show()   # don't be confused by the x-axis, the numbers reflect instances of numbers \n",
    "             # drawn between min_length and max_length with 0.1 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define participant\n",
    "Lucy = Kid('Lucy', 9, slope_of_function, point_of_reflection)\n",
    "\n",
    "# run experiment\n",
    "data = run_experiment(nblock,ntrial,Lucy)\n",
    "\n",
    "# show simulated raw data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance score\n",
    "\n",
    "# Score idea 1:\n",
    "def score1(data):\n",
    "    'returns a score computed by the difference in an assigned level and the proportion of wrong answers/block \\\n",
    "    data   = panda data frame'\n",
    "    \n",
    "    # compute level for each block\n",
    "    full_score = 0\n",
    "    for b in range(max(data['block'])+1):\n",
    "        block_data = data.query(f'block == {b}')\n",
    "        level      = block_data['level'][0]+1\n",
    "        score      = level - (1- sum(block_data['response']))/ntrial\n",
    "        full_score = full_score + score\n",
    "\n",
    "    exp_score = full_score/nblock\n",
    "    \n",
    "    return exp_score\n",
    "    \n",
    "# Score idea 2:\n",
    "\n",
    "def score2(data):\n",
    "    'returns the sum of correctly solved elements'\n",
    "    elements_correct = data['response']*data['length']\n",
    "    sum_correct = sum(elements_correct)\n",
    "    \n",
    "    return sum_correct/ntrial  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare score1 and score2 for different reflection points x0\n",
    "\n",
    "# shift x0 from 3 to 7, compute score1 and score2 and plot\n",
    "\n",
    "# Simulate for x experiments\n",
    "score_data = pd.DataFrame('NA', index=[], columns=['reflection_point', 'score1', 'score2'])\n",
    "slope      = 1\n",
    "repeats    = 100\n",
    "\n",
    "for point in range(3,8):\n",
    "    Baby = Kid('Baby',6, slope, point)\n",
    "    point_data = pd.DataFrame('NA', index=range(repeats), columns=['reflection_point', 'score1', 'score2'])\n",
    "    \n",
    "    for participant in range(repeats):\n",
    "        data = run_experiment(nblock,ntrial,Baby)\n",
    "        \n",
    "        s1   = score1(data)\n",
    "        s2   = score2(data)\n",
    "            \n",
    "        point_data.iloc[participant]['reflection_point'] = point\n",
    "        point_data.iloc[participant]['score1'] = s1\n",
    "        point_data.iloc[participant]['score2'] = s2\n",
    "               \n",
    "    score_data = pd.concat([score_data,point_data])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse output from simulated scores\n",
    "\n",
    "# mean score for each reflection level\n",
    "analyse_scores = pd.DataFrame('NA', index=[], columns=['reflection_point', 'mean_score1', 'mean_score2'])\n",
    "\n",
    "for point in range(3,8):\n",
    "    mean_scores = pd.DataFrame('NA', index=[0], columns=['reflection_point', 'mean_score1', 'mean_score2'])\n",
    "    subset         = score_data.query(f'reflection_point == {point}')\n",
    "    \n",
    "    mean_scores.iloc[0]['reflection_point'] = point\n",
    "    mean_scores.iloc[0]['mean_score1']      = sum(subset['score1'])/100\n",
    "    mean_scores.iloc[0]['mean_score2']      = sum(subset['score2'])/100\n",
    "    \n",
    "    analyse_scores = pd.concat([analyse_scores,mean_scores])\n",
    "\n",
    "plt.plot(analyse_scores['reflection_point'],analyse_scores['mean_score1']);\n",
    "plt.show()\n",
    "\n",
    "plt.plot(analyse_scores['reflection_point'],analyse_scores['mean_score2']);\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning over session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
